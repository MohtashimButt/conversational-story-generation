{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from enum import Enum, auto, unique\n",
    "from numbers import Number\n",
    "from typing import (Any, Dict, Iterable, List, Mapping, Optional, Sequence,\n",
    "                    Tuple, TypeVar, Union)\n",
    "\n",
    "import torch\n",
    "import zlib\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from itertools import islice, zip_longest\n",
    "from typing import (Any, Dict, Iterable, List, Optional, Sequence, Tuple,\n",
    "                    TypeVar, Union)\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer, BertTokenizer\n",
    "\n",
    "from datastructures import (\n",
    "    IndexedDict,\n",
    "    IndexedSet,\n",
    "    Trim,\n",
    ")\n",
    "\n",
    "from segments import (\n",
    "    Segment,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Special Tokens\n",
    " \n",
    " Special Tokens identify parts of narrative like characters and actions. In particular, we can sandwitch cards between special tokens unique to them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@unique\n",
    "class SpecialToken(str, Enum):\n",
    "    \"\"\"\n",
    "    An enumeration of special tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # This method must be defined before calling \"auto()\"\n",
    "    # pylint:disable=unused-argument,no-self-argument\n",
    "    def _generate_next_value_(name, start, count, last_values):\n",
    "        \"\"\"\n",
    "        Automatically create the enumeration name\n",
    "        \"\"\"\n",
    "        return f\"<|{name.upper()}|>\"\n",
    "\n",
    "    # pylint:enable=unused-argument,no-self-argument\n",
    "\n",
    "    @classmethod\n",
    "    def from_string(cls, name: str):\n",
    "        \"\"\"\n",
    "        Get the associated SpecialToken from the passed in string\n",
    "        \"\"\"\n",
    "        if name == \"name\":\n",
    "            # Handle this specially since Enum defines \"name\" as a string, but\n",
    "            # we want to use it to extract the field from the data\n",
    "            name = \"name_field\"\n",
    "        print(name)\n",
    "\n",
    "        return cls(f\"<|{name.upper()}|>\")\n",
    "\n",
    "    missing = auto()  # denotes missing information\n",
    "    separator = auto()  # separator token at the beginning of each Segment\n",
    "    character = auto()  # a character's biography\n",
    "\n",
    "    # All the possible card types from <CardNamespace> in the Storium export\n",
    "    # format: https://storium.com/help/export/json/0.9.2\n",
    "    chartype = auto()\n",
    "    goal = auto()\n",
    "    person = auto()\n",
    "    place = auto()\n",
    "    thing = auto()\n",
    "    strength = auto()\n",
    "    weakness = auto()\n",
    "    obstacle = auto()\n",
    "    subplot = auto()\n",
    "\n",
    "    # generic attributes for cards, characters, etc\n",
    "    name_field = auto()  # cannot call it \"name\", as Enum defines it as well\n",
    "    description = auto()\n",
    "\n",
    "    # Contextual card attributes\n",
    "    failure_stakes = auto()\n",
    "    success_stakes = auto()\n",
    "\n",
    "    # Information denoting entry type\n",
    "    move = auto()\n",
    "    establishment = auto()\n",
    "    addition = auto()\n",
    "    conclusion = auto()\n",
    "\n",
    "    # some notions of ordering\n",
    "    previous = auto()  # can stack, e.g. previous + previous => timestep t-2\n",
    "\n",
    "    # some notions of authorship\n",
    "    narrator = auto()  # can stack,  e.g. previous + narrator\n",
    "    same_character = auto()  # can stack,  e.g. previous + same_character\n",
    "    diff_character = auto()  #  can stack,  e.g. previous + diff_character\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Override the default string method to return the enumeration value,\n",
    "        which is a string\n",
    "        \"\"\"\n",
    "        return self.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_string(\n",
    "    field: str, mapping: Dict[str, Any], default: str = SpecialToken.missing.value\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Extract the given string field, accounting for the potential that it is\n",
    "    specified as None\n",
    "    \"\"\"\n",
    "    return mapping.get(field, default) or default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataclasses \n",
    "\n",
    "The code has dataclasses. Basically when our entire data would be processed, it would be in the form of these dataclasses. \n",
    "\n",
    "1. **ProcessedStory** - Our entire story would be in the form of this dataclass. It contains three things: \n",
    "    - *characters*\n",
    "    - *entries*\n",
    "    - *establishment_entries* \n",
    "- These would serve as our main cards. The entries and establishment entries would be in the form of **Entry Info** and character entries would be in the form of **Character Info**. Notice that they are in the form of IndexedDict.\n",
    "\n",
    "2. *Entry Info* -  Container for entries and establishment entries. \n",
    "2. *Character Info* - Container for characters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CharacterInfo:\n",
    "    \"\"\"\n",
    "    The processed character info\n",
    "    \"\"\"\n",
    "\n",
    "    # summary: Segment\n",
    "    name: str\n",
    "    description: str\n",
    "    character_id: str\n",
    "    checksum: int\n",
    "\n",
    "    # This is a sorted list of entry ids written by the character to\n",
    "    # allow easily looking up the previous entries for the character\n",
    "    entry_ids: IndexedSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EntryInfo:\n",
    "    \"\"\"\n",
    "    The processed entry info\n",
    "    \"\"\"\n",
    "\n",
    "    entry_id: str\n",
    "    character_id: str\n",
    "    establishment_id: str\n",
    "    checksum: int\n",
    "    text: str\n",
    "    imp_cards: Dict\n",
    "    format: str\n",
    "    # text: Segment\n",
    "    # summary: Segment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProcessedStory:\n",
    "    \"\"\"\n",
    "    This defines the structure of a story after processing\n",
    "    \"\"\"\n",
    "    \n",
    "    game_id: str\n",
    "\n",
    "    # A mapping of character id to character info\n",
    "    characters: IndexedDict[CharacterInfo]\n",
    "\n",
    "    # A mapping of entry id to entry info\n",
    "    entries: IndexedDict[EntryInfo]\n",
    "\n",
    "    # A mapping of entry id to establishment's entry info\n",
    "    establishment_entries: IndexedDict[EntryInfo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checksums \n",
    "\n",
    "For each card, there are checksums. This allows for selective reprocessing of data. Only the data that has changed (as indicated by a changed checksum) needs to be reprocessed and re-encoded for training. Hence, with entry associated card, we have a checksum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checksum_card(card: Optional[Dict[str, Any]], checksum: int = 1) -> int:\n",
    "    \"\"\"\n",
    "    Checksum the card.\n",
    "    \"\"\"\n",
    "    if not card:\n",
    "        return checksum\n",
    "\n",
    "    for field in (\"name\", \"description\", \"success_stakes\", \"failure_stakes\"):\n",
    "        checksum = zlib.adler32(\n",
    "            extract_string(field, card).encode(\"utf-8\"), checksum\n",
    "        )\n",
    "\n",
    "    return checksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checksum_cards(cards: List[Dict[str, Any]], checksum: int = 1) -> int:\n",
    "    \"\"\"\n",
    "    Create the summary of a card\n",
    "    \"\"\"\n",
    "    for card in cards:\n",
    "        checksum = checksum_card(card, checksum)\n",
    "\n",
    "    return checksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checksum_character(character: Dict[str, Any], character_id: str) -> int:\n",
    "    \"\"\"\n",
    "    Compute a checksum of a character\n",
    "    \"\"\"\n",
    "    checksum = zlib.adler32(character_id.encode(\"utf-8\"))\n",
    "    for field in (\"name\", \"description\"):\n",
    "        checksum = zlib.adler32(\n",
    "            extract_string(field, character).encode(\"utf-8\"), checksum\n",
    "        )\n",
    "\n",
    "    return checksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checksum_entry(entry: Dict[str, Any], entry_id: str) -> int:\n",
    "    \"\"\"\n",
    "    Compute a checksum of an entry\n",
    "    \"\"\"\n",
    "    checksum = zlib.adler32(entry_id.encode(\"utf-8\"))\n",
    "    entry_type = entry[\"format\"]\n",
    "    if entry_type == \"move\":\n",
    "        checksum = checksum_card(entry.get(\"target_challenge_card\"), checksum)\n",
    "        checksum = checksum_cards(\n",
    "            entry.get(\"cards_played_on_challenge\", []), checksum\n",
    "        )\n",
    "    elif entry_type == \"establishment\":\n",
    "        checksum = checksum_card(entry.get(\"place_card\"), checksum)\n",
    "    elif entry_type == \"addition\":\n",
    "        checksum = checksum_cards(entry.get(\"challenge_cards\", []), checksum)\n",
    "\n",
    "    return zlib.adler32(\n",
    "        extract_string(\"description\", entry, \"\").encode(\"utf-8\"), checksum\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The story details \n",
    "\n",
    "We now consider the main story processing. This is done by **process_story** function. The workflow of this function is the following: \n",
    "\n",
    "1. **Extract Scenes and Characters:** It starts by extracting scenes and characters from the story dictionary. If these are not present or not in the correct format, the function returns the processed object if it exists, effectively skipping processing.\n",
    "2. **Initialize Character List:**  A list of characters is initialized, starting with a default narrator character entry, which is always present in Storium stories but without a detailed summary (it has a checksum of 0, an empty entry_ids set, and an empty Segment as summary).\n",
    "\n",
    "3. **Process Characters:** Iterate over each character in the characters list. Generate a character_id from the character_seq_id and prefix it with character.\n",
    "\n",
    "4. **Process Scenes and Entries:** : Iterate over each scene in scenes, and within each, iterate over its entries. For each entry, compute its checksum and determine if it needs processing based on whether it has changed from the previously processed version. Process the entry using process_entry, which structures the entry's text, and associates it with the relevant character and scene information.\n",
    "\n",
    "5. **Construct ProcessedStory Object:** Compile the processed data into a ProcessedStory object, containing the structured data for the entire story, including mappings of characters and entries.\n",
    "\n",
    "The function utilizes another function called **process_entry**. The process_entry function is designed to process a single entry in a narrative or dataset, such as a character's action or a segment of a story, and encapsulate the processed data into an EntryInfo object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entry(\n",
    "    # tokenizer,\n",
    "    entry: Dict[str, Any],\n",
    "    establishment_id: str,\n",
    "    checksum: int,\n",
    "    add_eos: bool = True,\n",
    "    force: bool = False,\n",
    ") -> Optional[EntryInfo]:\n",
    "    \"\"\"\n",
    "    Process a character entry\n",
    "    \"\"\"\n",
    "    \n",
    "    text = extract_string(\"description\", entry, \"\")\n",
    "    entry_format = entry.get(\"format\")\n",
    "    if not text and not force and entry.get(\"format\") != \"establishment\":\n",
    "        # Only modeling moves with written text, though make a special\n",
    "        # exception for establishment entries. While they are currently\n",
    "        # required to have text, it seems at some point there were games that\n",
    "        # didn't have any text for the establishment entry, though it would still\n",
    "        # have place cards.\n",
    "        return None\n",
    "    \n",
    "    imp_cards = {}\n",
    "    # FOR CHARACTER MOVES\n",
    "    \n",
    "    imp_cards['target_challenge_card'] = [entry.get(\"target_challenge_card\")]\n",
    "    imp_cards['cards_played_on_challenge'] = entry.get(\"cards_played_on_challenge\")\n",
    "\n",
    "    # FOR SCENE CONTINUATION\n",
    "    imp_cards['challenge_cards'] = entry.get(\"challenge_cards\")\n",
    "\n",
    "    # PLACE WHEN SCENE STARTS (MIGHT BE NULL)\n",
    "    imp_cards[\"place_card\"]  = [entry.get(\"place_card\")]\n",
    "\n",
    "    return EntryInfo(\n",
    "        checksum=checksum,\n",
    "        entry_id=entry[\"seq_id\"],\n",
    "        character_id=entry[\"role\"],\n",
    "        establishment_id=establishment_id,\n",
    "        text=text,\n",
    "        format=entry_format,\n",
    "        # text=encoded_text,\n",
    "        imp_cards=imp_cards\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_story(story: Dict[str, Any], processed: Optional[ProcessedStory] = None):\n",
    "\n",
    "    scenes = story.get(\"scenes\")\n",
    "    characters = story.get(\"characters\")\n",
    "\n",
    "    # If either scenes or characters are missing, or scenes is not a proper sequence,\n",
    "    # we return previously processed data if available\n",
    "    if not scenes or not characters or not isinstance(scenes, Sequence):\n",
    "        return processed\n",
    "    \n",
    "    # We now create the character_list. To do this, we first sort the entry_ids using indexedSet(). The character_id\n",
    "    # is set to the narrator for the first character. \n",
    "\n",
    "    character_list = [\n",
    "        (\n",
    "            \"narrator\",\n",
    "            CharacterInfo(\n",
    "                name=\"narrator\",\n",
    "                description=\"\",\n",
    "                checksum=0, # setting narrator's checksum to 0\n",
    "                entry_ids=IndexedSet(),\n",
    "                character_id=\"narrator\", \n",
    "                # summary=Segment(),\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # =============================================================================== # \n",
    "    #                           Processing Character Entries \n",
    "    # ================================================================================#\n",
    "    \n",
    "    # We now Process each character in the story. We obtain the following:\n",
    "    # - Their ID, their associated checksum, their summary which is tokenized\n",
    "    # - Finally, we encapsulate all of it in the dataclass CharacterInfo. \n",
    "    for character in characters:\n",
    "        character_id = character.get(\"character_seq_id\")\n",
    "        if not character_id:\n",
    "            continue\n",
    "\n",
    "        character_id = f\"character:{character_id}\"\n",
    "\n",
    "        character_info = (\n",
    "            processed.characters.get(character_id, None) if processed else None\n",
    "        )\n",
    "        \n",
    "\n",
    "        # Compute the checksum for the character\n",
    "        checksum = checksum_character(character, character_id)\n",
    "        if not character_info or character_info.checksum != checksum:\n",
    "            # Haven't processed this character before, so process it now\n",
    "            character_info = CharacterInfo(\n",
    "                name=extract_string(\"name\", character),\n",
    "                description=extract_string(\"description\", character),\n",
    "                checksum=checksum,\n",
    "                entry_ids=IndexedSet(),\n",
    "                character_id=character_id,\n",
    "                # summary=summarize_character(character,tokenizer),\n",
    "            )\n",
    "\n",
    "        character_list.append(\n",
    "            (\n",
    "                character_id,\n",
    "                character_info,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    all_characters = IndexedDict(character_list)\n",
    "    \n",
    "    # =============================================================================== # \n",
    "    #                           Processing Scene Entries \n",
    "    # ================================================================================#    \n",
    "    \n",
    "    # same as characters. Obtain id, checksum and tokenized summaries and then encpasulate\n",
    "    # in dataclass entry_info. \n",
    "    \n",
    "    entry_list: List[Tuple[str, EntryInfo]] = []\n",
    "    establishment_list: List[Tuple[str, EntryInfo]] = []\n",
    "    \n",
    "    for scene in scenes:\n",
    "        entries = scene.get(\"entries\", [])\n",
    "        if not entries or not isinstance(entries, Sequence):\n",
    "            continue\n",
    "\n",
    "        for entry in entries:\n",
    "            entry_id = entry.get(\"seq_id\", None)\n",
    "            if entry_id is None:\n",
    "            \n",
    "                continue\n",
    "                \n",
    "            checksum = checksum_entry(entry, entry_id)\n",
    "            \n",
    "            entry_info = (\n",
    "                processed.entries.get(entry_id, None) if processed else None\n",
    "            )\n",
    "            if not entry_info or entry_info.checksum != checksum:\n",
    "                # Haven't processed this entry before, so process it now\n",
    "                entry_info = process_entry(\n",
    "                    # tokenizer,\n",
    "                    entry,\n",
    "                    establishment_list[-1][0] if establishment_list else entry_id,\n",
    "                    checksum,\n",
    "                )\n",
    "            if not entry_info:\n",
    "                continue\n",
    "\n",
    "            entry_list.append((entry_id, entry_info))\n",
    "            entry_format = entry.get(\"format\")\n",
    "            if entry_format == \"establishment\":\n",
    "                establishment_list.append((entry_id, entry_info))\n",
    "\n",
    "            character_info = (\n",
    "                all_characters[  # pylint:disable=unsubscriptable-object\n",
    "                    entry[\"role\"]\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            character_info.entry_ids.insert(entry_id)\n",
    "\n",
    "    return ProcessedStory(\n",
    "        game_id=story[\"game_pid\"],\n",
    "        entries=IndexedDict(entry_list), \n",
    "        characters=all_characters,\n",
    "        establishment_entries=IndexedDict(establishment_list),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorganize data for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(processed_story):\n",
    "    \"\"\"\n",
    "    Reorganize data for model and return it\n",
    "    Input: Processed Story\n",
    "    Output: Reformatted data for use\n",
    "    \"\"\"\n",
    "\n",
    "    first_scene_loc = list(processed_story.establishment_entries.values())[0].imp_cards['place_card'][0]\n",
    "    last_known_location = {\"card_id\": first_scene_loc[\"card_id\"], \"name\": first_scene_loc[\"name\"], \"description\": first_scene_loc[\"description\"]}  \n",
    "    bert_data = []\n",
    "\n",
    "    for entry_info in processed_story.entries.values():  # value is the entryInfo object\n",
    "\n",
    "        # when establishment entry\n",
    "        if entry_info.format == \"establishment\":\n",
    "            scene_location = entry_info.imp_cards[\"place_card\"][0]\n",
    "\n",
    "            if scene_location is not None:\n",
    "                last_known_location = {\"card_id\": scene_location[\"card_id\"], \"name\": scene_location[\"name\"], \"description\": scene_location[\"description\"]}\n",
    "            else:\n",
    "                # print(f\"entry_id: {entry_id} is establishment but has no place card, so using previous one\")\n",
    "                continue\n",
    "\n",
    "        elif entry_info.format == \"move\":\n",
    "\n",
    "            total_context = processed_story.establishment_entries[entry_info.establishment_id].text + entry_info.text\n",
    "            character = processed_story.characters[entry_info.character_id]\n",
    "\n",
    "            try:\n",
    "                    event = entry_info.imp_cards[\"cards_played_on_challenge\"][0]\n",
    "            except:\n",
    "                    # print(f\"{entry_info.entry_id} cards_played_on_challenge are null, skipping this entry\")\n",
    "                    continue\n",
    "\n",
    "            # making the data using the move entries\n",
    "            bert_data.append({\n",
    "                \"total_context\":total_context,\n",
    "                \"event\": {\"id\": event[\"card_id\"], \"name\": event['name'], \"description\": event['description']},\n",
    "                \"character\": {\"id\": character.character_id , \"name\": character.name, \"description\": character.description},\n",
    "                \"place\": {\"id\": last_known_location[\"card_id\"], \"name\": last_known_location[\"name\"], \"description\":last_known_location[\"description\"]},\n",
    "            })\n",
    "            \n",
    "        # scene conclusion and addition entries by narrator are ignored (since there is no event/character options to them for BERT)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return bert_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Training Data \n",
    "\n",
    "This creates a folder called \"bert_train_dataset\" which gives us all training samples for BERT. These were slightly modified to follow the format required for finetuning GPT-3.5 (the storyline guidance model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_folder = \"./storium_dataset\"  \n",
    "file_paths = []\n",
    "with open(f\"{data_folder}/train_filenames.txt\", \"r\") as filenames_file:\n",
    "    for line in filenames_file:\n",
    "        file_paths.append(line.strip())\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(f\"{data_folder}/{file_path}\", 'r', encoding='utf-8') as file:\n",
    "        story_data = json.load(file)\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    processed_story = process_story(story_data)\n",
    "    \n",
    "    # Skip stories with less than 5 characters (one is narrator)\n",
    "    num_chars = len(processed_story.characters)\n",
    "    if num_chars < 5:\n",
    "        continue  \n",
    "    else:\n",
    "        try:\n",
    "            bert_data = prepare_data(processed_story)\n",
    "        except Exception as e:\n",
    "            print(f\"{file_path} has the issue: {e}\")\n",
    "            continue  # Skip this story if there's an issue\n",
    "        \n",
    "        bert_data_folder_path = \"./bert_train_dataset\"\n",
    "        if not os.path.exists(bert_data_folder_path):\n",
    "            os.makedirs(bert_data_folder_path)\n",
    "            print(\"Directory created successfully.\")\n",
    "        \n",
    "        filename = file_path.split('/')[-1].rsplit('.', 1)[0] \n",
    "\n",
    "        try:\n",
    "            # Write the bert data to a JSON file\n",
    "            with open(f\"{bert_data_folder_path}/{filename}_bert_data.json\", \"w\") as json_file:\n",
    "                json.dump(bert_data, json_file, indent=4)\n",
    "            print(f\"Data written to {bert_data_folder_path}/{filename}_bert_data.json\")\n",
    "        except: \n",
    "            print(f\"{bert_data_folder_path}/{filename}_bert_data.json not opening\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
