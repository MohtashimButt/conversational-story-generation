{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d2d9a66",
   "metadata": {},
   "source": [
    "# Processing a single Story \n",
    "\n",
    "In this exposition, my main is to preprocess a single story in a way that we can send it to our model. Considering the complexity of the storium data, this is an extensive task. Let us see what it involves. Firstly, we have defined some useful datastructures that we would need for preprocessing in **datastructures.py**. Let me elaborate upon the nature of these. We have three of them: \n",
    "\n",
    "1. **Class Trim**\n",
    "2. **Class IndexedSet** \n",
    "3. **Class IndexedDic** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92989ab",
   "metadata": {},
   "source": [
    "### Imports & Datafiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92746f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import bisect\n",
    "import glob\n",
    "import heapq\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from enum import Enum, auto, unique\n",
    "from numbers import Number\n",
    "from typing import (Any, Dict, Iterable, List, Mapping, Optional, Sequence,\n",
    "                    Tuple, TypeVar, Union)\n",
    "\n",
    "import torch\n",
    "import zlib\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from itertools import islice, zip_longest\n",
    "from typing import (Any, Dict, Iterable, List, Optional, Sequence, Tuple,\n",
    "                    TypeVar, Union)\n",
    "\n",
    "from kiwisolver import Constraint, Solver, Variable, strength\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63949f00",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"5\" color=\"purple\"><b> Understanding Trim, IndexedDict and IndexedSet functionalities </b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "- The **trim class** defines how a data sequence should be trimmed it is exceeds a certain length. This is useful for us because we are going to be sending lengths which would far exceed the maximum capacity of the tokenizer (more than 1024 words for GPT-2. Hence, the trim class will be used to trim the segments which become too long \n",
    "    \n",
    "- The **IndexedSet** basically allows only unique elements to remain inside. However, it is distinct from the normal set class in that it allows for efficient indexing and insertion operations (so it maintains the order of the elements unlike the normal set dataclass)\n",
    "    \n",
    "- While a standard dictionary in Python is accessed via keys, **IndexedDict** allows acces via integer indices as well. \n",
    "\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed5045b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datastructures import (\n",
    "    IndexedDict,\n",
    "    IndexedSet,\n",
    "    Trim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d4380c",
   "metadata": {},
   "source": [
    "# Segments \n",
    "\n",
    "As we talked about in our previous exposition, the main uniqueness of the dataset are segments itself. These bring different cards together and effectively preprocesses them so that we can send them inside our model. Now, in data exploration, we talked about constraints and how these can be implemented using Cassoway Algorithm. Constraints are essential because they offer a way to control the length and contents of the segments. Since many of the models that we would be finetuning with allow a fixed context window, these constraints will allow us to flexibiliy change the context window. We don't need to worry about the creation of segments as the process itself is streamlined in the github repository associated with the dataset. A high-level overview would be fine for the project we are considering (which builds on the original work as we talked about).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95acc2cb",
   "metadata": {},
   "source": [
    "</ul>\n",
    "</p>\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"5\" color=\"purple\"><b>Overview of Segments </b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    " What we need to define are the following: \n",
    "    \n",
    "    \n",
    "- **Segment_ids** : these will associate a unique ID with each segment.\n",
    "- **EOS separator** : this will represent special tokens like a separator or end-of-sequence token.\n",
    "- **trim, constrained**, **naive_max_length**, **preferred_length**, **length:** will control how the segment is trimmed and constrained in length.\n",
    "\n",
    "As for constraint management, methods like hard_constraints, medium_constraints, and strong_constraints define various levels of constraints that can be applied to the segment's length, providing a flexible mechanism for controlling segment size and composition.\n",
    "\n",
    "\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d84b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segments import (\n",
    "    Segment,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82a8e3c",
   "metadata": {},
   "source": [
    "# Story Flow and Preprocessing\n",
    "\n",
    "These are essential parts of the code that we need to understand if we want to implement and build on the dataset as our project requires. Therefore, I have decided to bring many of the functionalities out of the general preprocessing class and see how each of the story is being preprocessed at every step of the way. On my way, I have modified some of the functions so that their logic is clear to everyone in the group.  My main aim with this is to see step-by-step how each story is being processed. This will ensure that we are familiar with the dataset to such extent that we would not have any difficulty when we actually implement the project.  \n",
    "\n",
    "Firstly, let me talk about the class **SpecialToken**. This one is more important than others conceptually. We know that during finetuning, we are going to sending across various cards such as **character cards**, **scenes*, **establishment** entries. The question arises as to how will our model be able to meaningfully seperate between these? (afterall at the end your just feeding numbers to the model. There has to be some correlation in those numbers that your model needs to pick on for it to meaningfully learn). \n",
    "\n",
    "Well, we can use special tokens to do this. Special Tokens identify parts of narrative like characters and actions. In particular, we can sandwitch cards between special tokens unique to them. For example, if we define a token like character, we will enclose it with the value *<|CHARACTER|>* \n",
    "... \n",
    "\n",
    "*The question of how effective this strategy is should be explored. That is, how best to demarcate our data so the model can best pick up on various correlations*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd41c341",
   "metadata": {},
   "outputs": [],
   "source": [
    "@unique\n",
    "class SpecialToken(str, Enum):\n",
    "    \"\"\"\n",
    "    An enumeration of special tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # This method must be defined before calling \"auto()\"\n",
    "    # pylint:disable=unused-argument,no-self-argument\n",
    "    def _generate_next_value_(name, start, count, last_values):\n",
    "        \"\"\"\n",
    "        Automatically create the enumeration name\n",
    "        \"\"\"\n",
    "        return f\"<|{name.upper()}|>\"\n",
    "\n",
    "    # pylint:enable=unused-argument,no-self-argument\n",
    "\n",
    "    @classmethod\n",
    "    def from_string(cls, name: str):\n",
    "        \"\"\"\n",
    "        Get the associated SpecialToken from the passed in string\n",
    "        \"\"\"\n",
    "        if name == \"name\":\n",
    "            # Handle this specially since Enum defines \"name\" as a string, but\n",
    "            # we want to use it to extract the field from the data\n",
    "            name = \"name_field\"\n",
    "\n",
    "        return cls(f\"<|{name.upper()}|>\")\n",
    "\n",
    "    missing = auto()  # denotes missing information\n",
    "    separator = auto()  # separator token at the beginning of each Segment\n",
    "    character = auto()  # a character's biography\n",
    "\n",
    "    # All the possible card types from <CardNamespace> in the Storium export\n",
    "    # format: https://storium.com/help/export/json/0.9.2\n",
    "    chartype = auto()\n",
    "    goal = auto()\n",
    "    person = auto()\n",
    "    place = auto()\n",
    "    thing = auto()\n",
    "    strength = auto()\n",
    "    weakness = auto()\n",
    "    obstacle = auto()\n",
    "    subplot = auto()\n",
    "\n",
    "    # generic attributes for cards, characters, etc\n",
    "    name_field = auto()  # cannot call it \"name\", as Enum defines it as well\n",
    "    description = auto()\n",
    "\n",
    "    # Contextual card attributes\n",
    "    failure_stakes = auto()\n",
    "    success_stakes = auto()\n",
    "\n",
    "    # Information denoting entry type\n",
    "    move = auto()\n",
    "    establishment = auto()\n",
    "    addition = auto()\n",
    "    conclusion = auto()\n",
    "\n",
    "    # some notions of ordering\n",
    "    previous = auto()  # can stack, e.g. previous + previous => timestep t-2\n",
    "\n",
    "    # some notions of authorship\n",
    "    narrator = auto()  # can stack,  e.g. previous + narrator\n",
    "    same_character = auto()  # can stack,  e.g. previous + same_character\n",
    "    diff_character = auto()  #  can stack,  e.g. previous + diff_character\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Override the default string method to return the enumeration value,\n",
    "        which is a string\n",
    "        \"\"\"\n",
    "        return self.value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2a3a5",
   "metadata": {},
   "source": [
    "# Encode & Encode Special function \n",
    "\n",
    "The **encode function** is pretty standard. You can think of it as performing normal tokenization as must be carried for all finetuning tasks. The authors have also added a functionality that suppress warnings related to text length exceeding the maximum length allowed by the tokenizer. However, all in all, its a pretty normal function. Let me briefly mention the **extract_string** function in this context. It is called again and again when we are tokenizing text. What it does is that it retrieves a string value from a dictionary based on a specified field name. If the field exists in the dictionary and its corresponding value is not None, the function returns that value as a string. \n",
    "\n",
    "\n",
    "**The Encode Special** is of significant interest. Basically, the *encode function extends the encode function by adding functionality to handle special tokens and create a Segment.* Hence, after tokenization has occured, the process of **segmentation** as discussed in the original paper is carried out by encode_special. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfcaf6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(\n",
    "    string_or_list: Union[str, List[str]],\n",
    "    tokenizer,\n",
    "    max_length: int\n",
    "\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    PreTrainedTokenizer.encode outputs warnings if the text being tokenized\n",
    "    is longer than the max_length specified in the tokenizer.\n",
    "    Unfortunately, the order of operations is to warn first, then truncate\n",
    "    to the max length that was passed in, resulting in spurious warnings,\n",
    "    so we wrap the function to suppress these warning messages.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(PreTrainedTokenizer.__module__)\n",
    "    log_level = logger.getEffectiveLevel()\n",
    "    logger.setLevel(logging.ERROR)\n",
    "\n",
    "    # Check if string_or_list is a list and join it if necessary\n",
    "    text_to_encode = \" \".join(string_or_list) if isinstance(string_or_list, list) else string_or_list\n",
    "    \n",
    "    # ================================ Unprint the line to see the text we are encoding ==================== # \n",
    "    \n",
    "    #print(text_to_encode)\n",
    "    \n",
    "    # ======================================================================================================\n",
    "    if max_length is not None:\n",
    "        tokens = tokenizer.encode(\n",
    "            text_to_encode , max_length=1024  \n",
    "        )\n",
    "    else:\n",
    "        tokens = tokenizer.encode(text_to_encode)\n",
    "        \n",
    "    logger.setLevel(log_level)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc45c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_special(\n",
    "    string_or_list: Union[str, List[str]],\n",
    "    tokenizer,\n",
    "    special_token: Optional[SpecialToken] = None,\n",
    "    separator_token_id: Optional[int] = None,\n",
    "    eos_token_id: Optional[int] = None,\n",
    "    preferred_length: int = 0,\n",
    "    trim: Trim = Trim.end,\n",
    ") -> Segment:\n",
    "        \"\"\"\n",
    "        After encoding with the tokenizer, this creates, create a Segment and\n",
    "        assign the special_token if specified.\n",
    "        \"\"\"\n",
    "        return Segment(\n",
    "            encode(string_or_list,tokenizer,1024), # WE NOW PASS TOKENIZER AND MAX LENGTH \n",
    "                                                    # EXPLICITLY\n",
    "            separator=tokenizer.convert_tokens_to_ids(SpecialToken.separator)\n",
    "            if separator_token_id is None\n",
    "            else separator_token_id,\n",
    "            eos=eos_token_id,\n",
    "            segment_ids=[tokenizer.convert_tokens_to_ids(special_token)]\n",
    "            if special_token\n",
    "            else tuple(),\n",
    "            preferred_length=preferred_length,\n",
    "            trim=trim,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcabb0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_string(\n",
    "    field: str, mapping: Dict[str, Any], default: str = SpecialToken.missing.value\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Extract the given string field, accounting for the potential that it is\n",
    "    specified as None\n",
    "    \"\"\"\n",
    "    # ============================================================= #\n",
    "    #print(\"FLOW OF COMPUTATION 19 \")\n",
    "    # ============================================================= #\n",
    "\n",
    "    return mapping.get(field, default) or default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4f426",
   "metadata": {},
   "source": [
    "# Dataclasses \n",
    "\n",
    "The code has dataclasses. Basically when our entire data would be processed, it would be in the form of these dataclasses. \n",
    "\n",
    "1. **ProcessedStory** - Our entire story would be in the form of this dataclass. It contains three things: \n",
    "    - *characters*\n",
    "    - *entries*\n",
    "    - *establishment_entries* \n",
    "- These would serve as our main cards. The entries and establishment entries would be in the form of **Entry Info** and character entries would be in the form of **Character Info**. Notice that they are in the form of IndexedDict.\n",
    "\n",
    "2. *Entry Info* -  Container for entries and establishment entries. \n",
    "2. *Character Info* - Container for characters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daa9c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CharacterInfo:\n",
    "    \"\"\"\n",
    "    The processed character info\n",
    "    \"\"\"\n",
    "    \n",
    "    # ============================================================= #\n",
    "    #print(\"Inside CharacterInfo\")\n",
    "    # ============================================================= #\n",
    "\n",
    "    summary: Segment\n",
    "    character_id: str\n",
    "    checksum: int\n",
    "\n",
    "    # This is a sorted list of entry ids written by the character to\n",
    "    # allow easily looking up the previous entries for the character\n",
    "    entry_ids: IndexedSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3971694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EntryInfo:\n",
    "    \"\"\"\n",
    "    The processed entry info\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    entry_id: str\n",
    "    character_id: str\n",
    "    establishment_id: str\n",
    "    checksum: int\n",
    "    text: Segment\n",
    "    summary: Segment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a074312",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProcessedStory:\n",
    "    \"\"\"\n",
    "    This defines the structure of a story after processing\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    game_id: str\n",
    "\n",
    "    # A mapping of character id to character info\n",
    "    characters: IndexedDict[CharacterInfo]\n",
    "\n",
    "    # A mapping of entry id to entry info\n",
    "    entries: IndexedDict[EntryInfo]\n",
    "\n",
    "    # A mapping of entry id to establishment's entry info\n",
    "    establishment_entries: IndexedDict[EntryInfo]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b5872",
   "metadata": {},
   "source": [
    "# Checksums \n",
    "\n",
    "For each card, there are checksums. This allows for selective reprocessing of data. Only the data that has changed (as indicated by a changed checksum) needs to be reprocessed and re-encoded for training. Hence, with entry associated card, we have a checksum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5aab72d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checksum_card(card: Optional[Dict[str, Any]], checksum: int = 1) -> int:\n",
    "    \"\"\"\n",
    "    Checksum the card.\n",
    "    \"\"\"\n",
    "    if not card:\n",
    "        return checksum\n",
    "\n",
    "    for field in (\"name\", \"description\", \"success_stakes\", \"failure_stakes\"):\n",
    "        checksum = zlib.adler32(\n",
    "            extract_string(field, card).encode(\"utf-8\"), checksum\n",
    "        )\n",
    "\n",
    "    return checksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d09ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checksum_cards(cards: List[Dict[str, Any]], checksum: int = 1) -> int:\n",
    "    \"\"\"\n",
    "    Create the summary of a card\n",
    "    \"\"\"\n",
    "    for card in cards:\n",
    "        checksum = checksum_card(card, checksum)\n",
    "\n",
    "    return checksum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3f13c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checksum_character(character: Dict[str, Any], character_id: str) -> int:\n",
    "    \"\"\"\n",
    "    Compute a checksum of a character\n",
    "    \"\"\"\n",
    "    checksum = zlib.adler32(character_id.encode(\"utf-8\"))\n",
    "    for field in (\"name\", \"description\"):\n",
    "        checksum = zlib.adler32(\n",
    "            extract_string(field, character).encode(\"utf-8\"), checksum\n",
    "        )\n",
    "\n",
    "    return checksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd809c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checksum_entry(entry: Dict[str, Any], entry_id: str) -> int:\n",
    "    \"\"\"\n",
    "    Compute a checksum of an entry\n",
    "    \"\"\"\n",
    "    checksum = zlib.adler32(entry_id.encode(\"utf-8\"))\n",
    "    entry_type = entry[\"format\"]\n",
    "    if entry_type == \"move\":\n",
    "        checksum = checksum_card(entry.get(\"target_challenge_card\"), checksum)\n",
    "        checksum = checksum_cards(\n",
    "            entry.get(\"cards_played_on_challenge\", []), checksum\n",
    "        )\n",
    "    elif entry_type == \"establishment\":\n",
    "        checksum = checksum_card(entry.get(\"place_card\"), checksum)\n",
    "    elif entry_type == \"addition\":\n",
    "        checksum = checksum_cards(entry.get(\"challenge_cards\", []), checksum)\n",
    "\n",
    "    return zlib.adler32(\n",
    "        extract_string(\"description\", entry, \"\").encode(\"utf-8\"), checksum\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9e85f",
   "metadata": {},
   "source": [
    "# Summaries of Cards\n",
    "\n",
    "These are the actual functions that preprocesses each entry and tokenizes them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "146d8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_character(character: Dict[str, Any], tokenizer) -> Segment:\n",
    "    \"\"\"\n",
    "    Create the summary for a character\n",
    "    \"\"\"\n",
    "    name_encoded = encode_special(\n",
    "        extract_string(\"name\", character),\n",
    "        tokenizer,\n",
    "        SpecialToken.from_string(\"name\"),\n",
    "        separator_token_id=tokenizer.bos_token_id,\n",
    "    )\n",
    "    \n",
    "    description_encoded = encode_special(\n",
    "        extract_string(\"description\", character),\n",
    "        tokenizer,\n",
    "        SpecialToken.from_string(\"description\"),\n",
    "    )\n",
    "    \n",
    "    encoded_fields = [name_encoded, description_encoded]\n",
    "    \n",
    "    return Segment(\n",
    "        iter(encoded_fields),\n",
    "        segment_ids=[tokenizer.convert_tokens_to_ids(SpecialToken.character)],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2792d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_card(tokenizer, card: Optional[Dict[str, Any]]) -> Segment:\n",
    "    \"\"\"\n",
    "    Create the summary of a card.\n",
    "\n",
    "    If it's a challenge card, then it'll have \"success_stakes\" and\n",
    "    \"failure_stakes\" as well.\n",
    "    \"\"\"\n",
    "    if not card:\n",
    "        return Segment()\n",
    "\n",
    "    return Segment(\n",
    "        iter(\n",
    "            encode_special(\n",
    "                string_or_list=extract_string(field, card),\n",
    "                tokenizer=tokenizer,\n",
    "                special_token=SpecialToken.from_string(field),\n",
    "            )\n",
    "            for field in (\"name\", \"description\", \"success_stakes\", \"failure_stakes\")\n",
    "            if card.get(field)\n",
    "        ),\n",
    "        segment_ids=tuple(\n",
    "            tokenizer.convert_tokens_to_ids(\n",
    "                (SpecialToken.from_string(card[\"namespace\"]),)\n",
    "            ),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de2113fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_cards(tokenizer,cards: List[Dict[str, Any]]) -> Segment:\n",
    "    \"\"\"\n",
    "    Create the summary of a card\n",
    "    \"\"\"\n",
    "    return Segment(iter(summarize_card(tokenizer,card) for card in cards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51d991f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_entry(tokenizer,entry: Dict[str, Any]) -> Segment:\n",
    "    \"\"\"\n",
    "    Create the summary of an entry\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "    entry_type = entry[\"format\"]\n",
    "    if entry_type == \"move\":\n",
    "        challenge = summarize_card(tokenizer,entry.get(\"target_challenge_card\"))\n",
    "        if challenge:\n",
    "            summary.append(challenge)\n",
    "\n",
    "        cards = summarize_cards(tokenizer,entry.get(\"cards_played_on_challenge\", []))\n",
    "        if cards:\n",
    "            summary.append(cards)\n",
    "    elif entry_type == \"establishment\":\n",
    "        place = summarize_card(tokenizer,entry.get(\"place_card\"))\n",
    "        if place:\n",
    "            summary.append(place)\n",
    "    elif entry_type == \"addition\":\n",
    "        cards = summarize_cards(tokenizer, entry.get(\"challenge_cards\", []))\n",
    "        if cards:\n",
    "            summary.append(cards)\n",
    "\n",
    "    return Segment(\n",
    "        summary,\n",
    "        segment_ids=tuple(\n",
    "            tokenizer.convert_tokens_to_ids(\n",
    "                (SpecialToken.from_string(entry_type),)\n",
    "            ),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a104bb",
   "metadata": {},
   "source": [
    "# The story details \n",
    "\n",
    "We now consider the main story processing. This is done by **process_story** function. The workflow of this function is the following:  \n",
    "1. **Extract Scenes and Characters:** It starts by extracting scenes and characters from the story dictionary. If these are not present or not in the correct format, the function returns the processed object if it exists, effectively skipping processing.\n",
    "2. **Initialize Character List:**  A list of characters is initialized, starting with a default narrator character entry, which is always present in Storium stories but without a detailed summary (it has a checksum of 0, an empty entry_ids set, and an empty Segment as summary).\n",
    "\n",
    "3. **Process Characters:** Iterate over each character in the characters list. Generate a character_id from the character_seq_id and prefix it with character:.\n",
    "\n",
    "4. **Process Scenes and Entries:** : Iterate over each scene in scenes, and within each, iterate over its entries. For each entry, compute its checksum and determine if it needs processing based on whether it has changed from the previously processed version. Process the entry using process_entry, which tokenizes and structures the entry's text, and associates it with the relevant character and scene information.\n",
    "\n",
    "5. **Construct ProcessedStory Object:** Compile the processed data into a ProcessedStory object, containing the structured data for the entire story, including mappings of characters and entries.\n",
    "\n",
    "The function utilizes another function called **process_entry**. The process_entry function is designed to process a single entry in a narrative or dataset, such as a character's action or a segment of a story, and encapsulate the processed data into an EntryInfo object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34d2a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now I set the preffered length as a hyperparameter manually. \n",
    "# It is set to be 256\n",
    "\n",
    "def process_entry(\n",
    "    tokenizer,\n",
    "    entry: Dict[str, Any],\n",
    "    establishment_id: str,\n",
    "    checksum: int,\n",
    "    add_eos: bool = True,\n",
    "    force: bool = False,\n",
    ") -> Optional[EntryInfo]:\n",
    "    \"\"\"\n",
    "    Process a character entry\n",
    "    \"\"\"\n",
    "    ###  SETTING HYPERPARAMETERS ON MY OWN ############\n",
    "    preferred_entry_length = 256 \n",
    "    \n",
    "    ############################################\n",
    "    \n",
    "    text = extract_string(\"description\", entry, \"\")\n",
    "    if not text and not force and entry.get(\"format\") != \"establishment\":\n",
    "        # Only modeling moves with written text, though make a special\n",
    "        # exception for establishment entries. While they are currently\n",
    "        # required to have text, it seems at some point there were games that\n",
    "        # didn't have any text for the establishment entry, though it would still\n",
    "        # have place cards.\n",
    "        return None\n",
    "\n",
    "    \n",
    "    encoded_text = encode_special(\n",
    "        text,\n",
    "        tokenizer=tokenizer,\n",
    "        special_token=SpecialToken.from_string(entry[\"format\"]),\n",
    "        preferred_length=preferred_entry_length,\n",
    "        eos_token_id=tokenizer.eos_token_id if add_eos else None,\n",
    "    )\n",
    "    summary = summarize_entry(tokenizer,entry)\n",
    "    if not summary:\n",
    "        summary = encode_special(\n",
    "            string_or_list=text,\n",
    "            tokenizer=tokenizer,\n",
    "            special_token=SpecialToken.from_string(entry[\"format\"]),\n",
    "            trim=Trim.start,  # Treat the end of the entry text as a summary\n",
    "        )\n",
    "\n",
    "    return EntryInfo(\n",
    "        checksum=checksum,\n",
    "        entry_id=entry[\"seq_id\"],\n",
    "        character_id=entry[\"role\"],\n",
    "        establishment_id=establishment_id,\n",
    "        text=encoded_text,\n",
    "        summary=summary,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9aa41",
   "metadata": {},
   "source": [
    "# Process_story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db566cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_story(story: Dict[str, Any], tokenizer, processed: Optional[ProcessedStory] = None) -> Optional[ProcessedStory]:\n",
    "\n",
    "\n",
    "    # HERE WE OBTAIN THE SCENES \n",
    "    scenes = story.get(\"scenes\")\n",
    "    print(\"the total number of scenes are \", len(scenes))\n",
    "    \n",
    "    # HERE WE OBTAIN ALL THE CHARACTERS \n",
    "    characters = story.get(\"characters\")\n",
    "    print(\"the total number of characters are \", len(scenes))\n",
    "\n",
    "    # If either scenes or characters are missing, or scenes is not a proper sequence,\n",
    "    # we return previously processed data if available\n",
    "    if not scenes or not characters or not isinstance(scenes, Sequence):\n",
    "        return processed\n",
    "    \n",
    "    #We now create the character_list. To do this, we first sort the entry_ids using indexedSet(). The character_id\n",
    "    # is set to the narrative. To obtain the actual contents which is found in summary, we send it to the Segment ()\n",
    "    # class. \n",
    "\n",
    "    character_list = [\n",
    "        # Treat narrator as a character who is always present without a summary\n",
    "        (\n",
    "            \"narrator\",\n",
    "            CharacterInfo(\n",
    "                checksum=0,\n",
    "                entry_ids=IndexedSet(),\n",
    "                character_id=\"narrator\",\n",
    "                summary=Segment(),\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # =============================================================================== # \n",
    "    #                           Processing Character Entries \n",
    "    # ================================================================================#\n",
    "    \n",
    "    # We now Process each character in the story. We obtain the following:\n",
    "    # - Their ID, their associated checksum, their summary which is tokenized\n",
    "    # - Finally, we encapsulate all of it in the dataclass CharacterInfo. \n",
    "    for character in characters:\n",
    "        character_id = character.get(\"character_seq_id\")\n",
    "        if not character_id:\n",
    "            continue\n",
    "\n",
    "\n",
    "        character_id = f\"character:{character_id}\"\n",
    "\n",
    "        character_info = (\n",
    "            processed.characters.get(character_id, None) if processed else None\n",
    "        )\n",
    "        \n",
    "\n",
    "        # Compute the checksum for the character\n",
    "        checksum = checksum_character(character, character_id)\n",
    "        if not character_info or character_info.checksum != checksum:\n",
    "            # Haven't processed this character before, so process it now\n",
    "            character_info = CharacterInfo(\n",
    "                checksum=checksum,\n",
    "                entry_ids=IndexedSet(),\n",
    "                character_id=character_id,\n",
    "                summary=summarize_character(character,tokenizer),\n",
    "            )\n",
    "\n",
    "        character_list.append(\n",
    "            (\n",
    "                character_id,\n",
    "                character_info,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "    all_characters = IndexedDict(character_list)\n",
    "    \n",
    "    # =============================================================================== # \n",
    "    #                           Processing Scene Entries \n",
    "    # ================================================================================#    \n",
    "    \n",
    "    # same as characters. Obtain id, checksum and tokenized summaries and then encpasulate\n",
    "    # in dataclass entry_info. \n",
    "    \n",
    "    entry_list: List[Tuple[str, EntryInfo]] = []\n",
    "    establishment_list: List[Tuple[str, EntryInfo]] = []\n",
    "    for scene in scenes:\n",
    "        entries = scene.get(\"entries\", [])\n",
    "        if not entries or not isinstance(entries, Sequence):\n",
    "            continue\n",
    "\n",
    "        for entry in entries:\n",
    "            entry_id = entry.get(\"seq_id\", None)\n",
    "            if entry_id is None:\n",
    "            \n",
    "                continue\n",
    "                \n",
    "\n",
    "            \n",
    "            checksum = checksum_entry(entry, entry_id)\n",
    "            \n",
    "\n",
    "            entry_info = (\n",
    "                processed.entries.get(entry_id, None) if processed else None\n",
    "            )\n",
    "            if not entry_info or entry_info.checksum != checksum:\n",
    "                # Haven't processed this entry before, so process it now\n",
    "                entry_info = process_entry(\n",
    "                    tokenizer,\n",
    "                    entry,\n",
    "                    establishment_list[-1][0] if establishment_list else entry_id,\n",
    "                    checksum,\n",
    "                )\n",
    "            if not entry_info:\n",
    "                continue\n",
    "\n",
    "            entry_list.append((entry_id, entry_info))\n",
    "            entry_format = entry.get(\"format\")\n",
    "            if entry_format == \"establishment\":\n",
    "                establishment_list.append((entry_id, entry_info))\n",
    "\n",
    "            character_info = (\n",
    "                all_characters[  # pylint:disable=unsubscriptable-object\n",
    "                    entry[\"role\"]\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            character_info.entry_ids.insert(entry_id)\n",
    "\n",
    "\n",
    "    return ProcessedStory(\n",
    "        game_id=story[\"game_pid\"],\n",
    "        entries=IndexedDict(entry_list),\n",
    "        characters=all_characters,\n",
    "        establishment_entries=IndexedDict(establishment_list),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ca8ea5",
   "metadata": {},
   "source": [
    "# Preprocessing a single story \n",
    "\n",
    "*To be done. We will do a before vs after of preprocessed story. For now I am just showing how to feed a story inside the function. Hopefully, we will be able to finetune soon. Apologies for the delay but work has been alot and the dataset has really taken alot of time to understand. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77029315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset_path = r'C:\\Users\\AWCD\\OneDrive\\Desktop\\CS438 Generative AI\\Project\\storium_2019_08_22'    \n",
    "file_path = '/full_export/5/9/591cca.json'\n",
    "total_path = dataset_path + file_path\n",
    "\n",
    "with open(total_path, 'r',encoding='utf-8') as file:\n",
    "    story_data = json.load(file) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0054203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>\n",
      "the total number of scenes are  54\n",
      "the total number of characters are  54\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "print(type(tokenizer))\n",
    "\n",
    "\n",
    "processed_story = process_story(story_data, tokenizer) # as talked about \n",
    "                                              # we would now need to explicitly \n",
    "                                              # pass tokenizer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
