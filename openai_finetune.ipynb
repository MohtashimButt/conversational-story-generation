{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "\n",
    "    new_data = []\n",
    "    data_length = len(data)\n",
    "    print(\"DATA LENGTH: \",data_length)\n",
    "    for i in range(data_length-4): # since we have next 3 events as the wrong choices, and next entities as right choices\n",
    "        correct_set = data[i]\n",
    "\n",
    "        # remove all punctuation except periods, commas, and apostrophes\n",
    "        pattern = r'[^\\w\\s.,\\']'\n",
    "\n",
    "        total_context = re.sub(pattern, '', correct_set['total_context']) if correct_set['total_context'] else \"\"\n",
    "\n",
    "\n",
    "        # remove multiple spaces (causing issues with tiktoken)\n",
    "        event_desc_i = re.sub(pattern, '', data[i+1]['event']['description']) if data[i+1]['event']['description'] else \"\"\n",
    "        character_desc_i = re.sub(pattern, '',  data[i+1]['character']['description']) if data[i+1]['character']['description'] else \"\"\n",
    "        place_desc_i = re.sub(pattern, '',  data[i+1]['place']['description']) if data[i+1]['place']['description'] else \"\"\n",
    "\n",
    "        event_desc_i_next = re.sub(pattern, '', data[i+2]['event']['description']) if data[i+2]['event']['description'] else \"\"\n",
    "        character_desc_i_next = re.sub(pattern, '', data[i+2]['character']['description']) if data[i+2]['character']['description'] else \"\"\n",
    "        place_desc_i_next = re.sub(pattern, '', data[i+2]['place']['description']) if data[i+2]['place']['description'] else \"\"\n",
    "\n",
    "        event_desc_i_next2 = re.sub(pattern, '', data[i+3]['event']['description']) if data[i+3]['event']['description'] else \"\"\n",
    "        character_desc_i_next2 = re.sub(pattern, '', data[i+3]['character']['description']) if data[i+3]['character']['description'] else \"\"\n",
    "        place_desc_i_next2 = re.sub(pattern, '', data[i+3]['place']['description']) if data[i+3]['place']['description'] else \"\"\n",
    "\n",
    "        event_desc_i_next3 = re.sub(pattern, '', data[i+4]['event']['description']) if data[i+4]['event']['description'] else \"\"\n",
    "        character_desc_i_next3 = re.sub(pattern, '', data[i+4]['character']['description']) if data[i+4]['character']['description'] else \"\"\n",
    "        place_desc_i_next3 = re.sub(pattern, '', data[i+4]['place']['description']) if data[i+4]['place']['description'] else \"\"\n",
    "\n",
    "\n",
    "        new_data.append({\n",
    "            \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": total_context,\n",
    "            }, \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the next event in this context? Who is the main character in that event? What is the scene location?\"\n",
    "            },\n",
    "\n",
    "            # correct answer set\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \n",
    "                  f\"<event_name>{data[i+1]['event']['name']}</event_name>\"\n",
    "                  f\"<event_description>{event_desc_i}</event_description>\"\n",
    "                  f\"<character_name>{data[i+1]['character']['name']}</character_name>\"\n",
    "                  f\"<character_description>{character_desc_i}</character_description>\"\n",
    "                  f\"<place_name>{data[i+1]['place']['name']}</place_name>\"\n",
    "                  f\"<place_description>{place_desc_i}</place_description>\"\n",
    "                ,\n",
    "                \"weight\": 1\n",
    "            },\n",
    "\n",
    "            # incorrect answer sets\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \n",
    "                    f\"<event_name>{data[i+2]['event']['name'].strip()}</event_name>\"\n",
    "                    f\"<event_description>{event_desc_i_next}</event_description>\"\n",
    "                    f\"<character_name>{data[i+2]['character']['name'].strip()}</character_name>\"\n",
    "                    f\"<character_description>{character_desc_i_next}</character_description>\"\n",
    "                    f\"<place_name>{data[i+2]['place']['name'].strip()}</place_name>\"\n",
    "                    f\"<place_description>{place_desc_i_next}</place_description>\"\n",
    "                ,\n",
    "                \"weight\": 0\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \n",
    "                   f\"<event_name>{data[i+3]['event']['name']}</event_name>\"\n",
    "                   f\"<event_description>{event_desc_i_next2}</event_description>\"\n",
    "                   f\"<character_name>{data[i+3]['character']['name']}</character_name>\"\n",
    "                   f\"<character_description>{character_desc_i_next2}</character_description>\"\n",
    "                   f\"<place_name>{data[i+3]['place']['name']}</place_name>\"\n",
    "                   f\"<place_description>{place_desc_i_next2}</place_description>\"\n",
    "                ,\n",
    "                \"weight\": 0\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \n",
    "                    f\"<event_name>{data[i+4]['event']['name']}</event_name>\"\n",
    "                    f\"<event_description>{event_desc_i_next3}</event_description>\"\n",
    "                    f\"<character_name>{data[i+4]['character']['name']}</character_name>\"\n",
    "                    f\"<character_description>{character_desc_i_next3}</character_description>\"\n",
    "                    f\"<place_name>{data[i+4]['place']['name']}</place_name>\"\n",
    "                    f\"<place_description>{place_desc_i_next3}</place_description>\"\n",
    "                ,\n",
    "                \"weight\": 0\n",
    "            }]})\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make training data as required by OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LENGTH:  79\n",
      "File './openai_train_dataset/0pzw4q_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  11\n",
      "File './openai_train_dataset/1mpsvd_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  26\n",
      "File './openai_train_dataset/1kynba_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  59\n",
      "File './openai_train_dataset/0tvryd_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  19\n",
      "File './openai_train_dataset/1cq700_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  38\n",
      "File './openai_train_dataset/0rz2vg_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  21\n",
      "File './openai_train_dataset/0d4d57_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  59\n",
      "File './openai_train_dataset/0qy2qe_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  24\n",
      "File './openai_train_dataset/0dgp9x_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  33\n",
      "File './openai_train_dataset/0gzcr1_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  15\n",
      "File './openai_train_dataset/1rnyh8_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  49\n",
      "File './openai_train_dataset/1qbng7_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  29\n",
      "File './openai_train_dataset/0qse99_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  32\n",
      "File './openai_train_dataset/0fwk70_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  11\n",
      "File './openai_train_dataset/1gaagh_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  30\n",
      "File './openai_train_dataset/1pqj4r_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  32\n",
      "File './openai_train_dataset/0qxn4h_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  150\n",
      "File './openai_train_dataset/0h1y1y_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  51\n",
      "File './openai_train_dataset/1dh5vn_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  20\n",
      "File './openai_train_dataset/0shdw0_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  54\n",
      "File './openai_train_dataset/1t23cd_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  32\n",
      "File './openai_train_dataset/0syapk_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  15\n",
      "File './openai_train_dataset/1keseb_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  45\n",
      "File './openai_train_dataset/0kvkn5_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  39\n",
      "File './openai_train_dataset/0hc4vm_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  106\n",
      "File './openai_train_dataset/0bykeg_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  214\n",
      "File './openai_train_dataset/1qaw0a_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  38\n",
      "File './openai_train_dataset/0zaazn_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  0\n",
      "File './openai_train_dataset/0smv44_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  41\n",
      "File './openai_train_dataset/00dg9y_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  64\n",
      "File './openai_train_dataset/1ne2tn_openai_data.jsonl' created successfully.\n",
      "DATA LENGTH:  130\n",
      "File './openai_train_dataset/0r48tt_openai_data.jsonl' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# bert_dataset_folder = \"./bert_train_dataset\"\n",
    "bert_train_dataset_folder = \"./bert_sample\"\n",
    "openai_train_dataset_folder = \"./openai_train_dataset\"\n",
    "\n",
    "if not os.path.exists(openai_train_dataset_folder):\n",
    "    os.makedirs(openai_train_dataset_folder)\n",
    "    print(\"Directory 'openai_train_dataset' created successfully.\")\n",
    "\n",
    "for filename in os.listdir(bert_train_dataset_folder):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(bert_train_dataset_folder, filename)\n",
    "        new_file_path = os.path.join(openai_train_dataset_folder, filename.replace(\"_bert_data.json\", \"_openai_data.jsonl\"))\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            prev_data = json.load(file)\n",
    "            new_data = prepare_data(prev_data)\n",
    "        \n",
    "        if new_data is not None:\n",
    "            with open(new_file_path, \"w\") as jsonl_file:\n",
    "                for item in new_data:\n",
    "                    json.dump(item, jsonl_file)\n",
    "                    jsonl_file.write('\\n')\n",
    "\n",
    "        print(f\"File '{new_file_path}' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify data format correctness\n",
    "\n",
    "Verify if data is according to the format required by OpenAI for finetuning GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_train_dataset_folder = \"./openai_train_dataset\"\n",
    "files = os.listdir(openai_train_dataset_folder)\n",
    "\n",
    "if files:\n",
    "    first_file_path = os.path.join(openai_train_dataset_folder, files[0])\n",
    "\n",
    "    # Load the dataset from the first file\n",
    "    with open(first_file_path, 'r', encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    # Initial dataset stats\n",
    "    print(\"Num examples:\", len(dataset))\n",
    "    print(\"First example:\")\n",
    "    for message in dataset[0][\"messages\"]:\n",
    "        print(message)\n",
    "else:\n",
    "    print(\"No files found in the folder:\", openai_train_dataset_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            if key == \"weight\":\n",
    "                    continue\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 6, 6\n",
      "mean / median: 6.0, 6.0\n",
      "p5 / p95: 6.0, 6.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 21, 21\n",
      "mean / median: 21.0, 21.0\n",
      "p5 / p95: 21.0, 21.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 3005, 6532\n",
      "mean / median: 4997.875, 4829.5\n",
      "p5 / p95: 3031.6, 6529.2\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~168 tokens that will be charged for during training\n",
      "By default, you'll train for 12 epochs on this dataset\n",
      "By default, you'll be charged for ~2016 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning .... finally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "API_KEY = \"sk-proj-JYHoORcm3bg4YuzFKDNsT3BlbkFJTCr1mK3WwSSlzkPJ1eMV\"\n",
    "\n",
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File './openai_train_dataset/1keseb_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/1t23cd_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/1kynba_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0r48tt_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0bykeg_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/00dg9y_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/1gaagh_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0syapk_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/1rnyh8_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0kvkn5_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/1qaw0a_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/1qbng7_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0dgp9x_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0rz2vg_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0qse99_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0qxn4h_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/1cq700_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/1ne2tn_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0zaazn_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0tvryd_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/1pqj4r_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0shdw0_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0d4d57_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0pzw4q_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/1mpsvd_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0qy2qe_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0gzcr1_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0hc4vm_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0h1y1y_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/1dh5vn_openai_data.jsonl' uploaded successfully.\n",
      "File './openai_train_dataset/0fwk70_openai_data.jsonl' uploaded successfully.\n"
     ]
    }
   ],
   "source": [
    "openai_train_dataset_folder = \"./openai_train_dataset\"\n",
    "files = os.listdir(openai_train_dataset_folder)\n",
    "training_file_IDs = []\n",
    "\n",
    "# Loop through each file path and upload the file\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(openai_train_dataset_folder, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            try:\n",
    "                training_file_IDs.append(client.files.create(\n",
    "                    file=file,\n",
    "                    purpose=\"fine-tune\"\n",
    "                ))\n",
    "            except:\n",
    "                continue\n",
    "        print(f\"File '{file_path}' uploaded successfully.\")\n",
    "    else:\n",
    "        print(f\"File '{file_path}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a sample job + checking it\n",
    "\n",
    "We can see that the job has been successful since there are no errors in the \"error\" field. Uploaded one file because it took $6 per file to finetune. We can't pay anymore than we have to to show proof of work lol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the keys to access the FineTuning Object:  dict_keys(['id', 'created_at', 'error', 'fine_tuned_model', 'finished_at', 'hyperparameters', 'model', 'object', 'organization_id', 'result_files', 'seed', 'status', 'trained_tokens', 'training_file', 'validation_file', 'estimated_finish', 'integrations'])\n",
      "Error(code=None, message=None, param=None)\n",
      "**Next Event**:  \n",
      "The main character checks the bottom of the bowl for any leftover food.\n",
      "\n",
      "\n",
      "**Main Character**:  \n",
      "**Name:**  \tAnna\n",
      "**Role:**  \tThe main character\n",
      "\n",
      "\n",
      "**Scene Location:**  \n",
      "**Name:**  \tthe living room\n",
      "**Description:**  \tA room in the apartment with mismatched but cozy furniture. There is a large window letting in plenty of natural light.\n"
     ]
    }
   ],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_IDs[0].id,\n",
    "    model=\"gpt-3.5-turbo-0125\"\n",
    ")\n",
    "\n",
    "fine_tuning_jobs = client.fine_tuning.jobs.list(limit=10)\n",
    "keys = fine_tuning_jobs.data[0].__annotations__.keys()\n",
    "print(\"Here are the keys to access the FineTuning Object: \", keys)\n",
    "\n",
    "# see that there is no error\n",
    "print(fine_tuning_jobs.data[0].error)\n",
    "\n",
    "# get the fine-tuned model\n",
    "model = fine_tuning_jobs.data[0].fine_tuned_model\n",
    "if model is not None:\n",
    "    completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"}, # the output of gpt-3.5 (Storyline Generation) model will be the content\n",
    "        {\"role\": \"user\", \"content\": \"What is the next event in this context? Who is the main character in that event? What is the scene location? \"}\n",
    "    ]\n",
    "    )\n",
    "answer = completion.choices[0].message.content # this answer will guide the other GPT-3.5 (Story Generation) model\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note about more training data\n",
    "\n",
    "The above code would apply with minor modifications had we sent more training files to the model. But since we don't have the resources (money) this was not done. The rest of the steps for getting the model etc stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning job for each uploaded file\n",
    "# for fileObj in training_file_IDs:\n",
    "#     client.fine_tuning.jobs.create(\n",
    "#         training_file=fileObj.id,\n",
    "#         model=\"gpt-3.5-turbo-0125\"\n",
    "#     )\n",
    "#     print(f\"Fine-tuning job created for file ID: {fileObj.id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
